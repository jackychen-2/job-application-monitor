# Job Application Monitor â€” Production Architecture

## 1. Overview

Transform the current single-file script (`monitor_job_apps.py`, 711 lines) into a modular, testable, cross-platform Python application with a proper database, web dashboard, CLI interface, and clean separation of concerns â€” while preserving all existing functionality.

### Current State Problems

| Problem | Impact |
|---------|--------|
| Single 711-line monolithic script | Hard to test, maintain, extend |
| macOS-only (Numbers.app via AppleScript) | Not cross-platform |
| `print()` for logging | No log levels, no file logging, no structured output |
| JSON file for state tracking | No duplicate detection, no history, fragile |
| Broad try/except in `main()` | Silent failures, hard to debug |
| No tests | Regressions go undetected |
| No web interface | Can only view data in Numbers spreadsheet |
| Regex-heavy extraction | Fragile, hard to tune |
| No retry logic for IMAP/LLM | Fails on transient network errors |
| Hardcoded LLM provider | Cannot swap to other providers |
| No data export flexibility | Only Numbers format |

### Design Principles

- **MVP-first**: Ship a working personal tool, but design for multi-user expansion
- **Cross-platform**: Drop macOS-only dependencies from the core; keep Numbers export as optional
- **Testable**: Every module independently testable with dependency injection
- **Configurable**: Pydantic-validated configuration, environment-driven
- **Observable**: Structured logging with levels, optional cost tracking
- **Web-native**: Dashboard for viewing and managing applications

---

## 2. System Architecture

```mermaid
flowchart TB
    subgraph Frontend
        WEB[React SPA<br/>Dashboard + Filters + Stats]
    end
    
    subgraph Backend
        API[FastAPI Server<br/>REST API]
        SCAN[Email Scanner<br/>Background Task]
        CLI[Typer CLI<br/>Command Line]
    end
    
    subgraph Core
        CFG[Config<br/>Pydantic Settings]
        PIPE[Extraction Pipeline<br/>Rules + LLM]
        IMAP[IMAP Client<br/>With Retry]
        PARSE[Email Parser<br/>MIME Decoder]
        CLASS[Classifier<br/>Job Detection]
        EXP[Exporter<br/>CSV + Excel]
    end
    
    subgraph Data
        DB[(SQLite DB<br/>Applications + History)]
    end
    
    WEB -->|HTTP| API
    API --> DB
    API --> SCAN
    CLI --> SCAN
    CLI --> DB
    CLI --> EXP
    SCAN --> IMAP
    SCAN --> PARSE
    SCAN --> CLASS
    SCAN --> PIPE
    SCAN --> DB
    PIPE --> DB
    API --> EXP
```

---

## 3. Project Structure

```
job-monitor/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ job_monitor/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py                   # FastAPI app entry point
â”‚       â”œâ”€â”€ config.py                 # Pydantic settings and validation
â”‚       â”œâ”€â”€ models.py                 # SQLAlchemy ORM models
â”‚       â”œâ”€â”€ schemas.py                # Pydantic API request/response schemas
â”‚       â”œâ”€â”€ database.py              # DB engine, session, init
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ applications.py      # CRUD endpoints for applications
â”‚       â”‚   â”œâ”€â”€ scan.py              # Trigger scan endpoint
â”‚       â”‚   â”œâ”€â”€ stats.py             # Dashboard statistics endpoint
â”‚       â”‚   â””â”€â”€ export.py            # Export endpoint (CSV/Excel download)
â”‚       â”œâ”€â”€ email/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ client.py            # IMAP client with retry and timeout
â”‚       â”‚   â”œâ”€â”€ parser.py            # MIME decoding, body extraction
â”‚       â”‚   â””â”€â”€ classifier.py        # Keyword-based job classification
â”‚       â”œâ”€â”€ extraction/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ rules.py             # Regex-based field extraction
â”‚       â”‚   â”œâ”€â”€ llm.py               # LLM provider abstraction
â”‚       â”‚   â””â”€â”€ pipeline.py          # Orchestrator: rules + LLM fallback
â”‚       â”œâ”€â”€ export/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ csv_export.py        # CSV export
â”‚       â”‚   â””â”€â”€ excel_export.py      # Excel export via openpyxl
â”‚       â”œâ”€â”€ cli.py                    # Typer CLI entry point
â”‚       â””â”€â”€ logging_config.py        # Structured logging setup
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts               # Vite build config
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.tsx                  # React entry point
â”‚       â”œâ”€â”€ App.tsx                   # Root component with routing
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â””â”€â”€ client.ts            # API client (fetch wrapper)
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ Layout.tsx            # Page layout with nav
â”‚       â”‚   â”œâ”€â”€ ApplicationTable.tsx  # Main data table with sorting
â”‚       â”‚   â”œâ”€â”€ StatusBadge.tsx       # Color-coded status badges
â”‚       â”‚   â”œâ”€â”€ FilterBar.tsx         # Status/company/date filters
â”‚       â”‚   â”œâ”€â”€ StatsCards.tsx        # Summary stat cards
â”‚       â”‚   â”œâ”€â”€ StatusChart.tsx       # Status distribution chart
â”‚       â”‚   â””â”€â”€ ScanButton.tsx        # Trigger email scan
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ Dashboard.tsx         # Main dashboard page
â”‚       â”‚   â””â”€â”€ ApplicationDetail.tsx # Single application detail + history
â”‚       â”œâ”€â”€ types/
â”‚       â”‚   â””â”€â”€ index.ts             # TypeScript interfaces
â”‚       â””â”€â”€ styles/
â”‚           â””â”€â”€ index.css             # Tailwind CSS
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                  # Fixtures: in-memory DB, mock IMAP
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_email_client.py
â”‚   â”œâ”€â”€ test_email_parser.py
â”‚   â”œâ”€â”€ test_classifier.py
â”‚   â”œâ”€â”€ test_extraction_rules.py
â”‚   â”œâ”€â”€ test_llm_extraction.py
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â”œâ”€â”€ test_database.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_export.py
â”œâ”€â”€ alembic/                         # Database migrations
â”‚   â”œâ”€â”€ env.py
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ pyproject.toml                   # Python project config
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ Makefile
```

---

## 4. Database Schema

SQLite for MVP. Designed to migrate to PostgreSQL with zero code changes via SQLAlchemy.

```mermaid
erDiagram
    applications {
        int id PK
        text company
        text job_title
        text email_subject
        text email_sender
        timestamp email_date
        text status
        text source
        text notes
        timestamp created_at
        timestamp updated_at
    }
    
    status_history {
        int id PK
        int application_id FK
        text old_status
        text new_status
        text change_source
        timestamp changed_at
    }
    
    processed_emails {
        int id PK
        int uid
        text email_account
        text email_folder
        text subject
        text sender
        timestamp email_date
        boolean is_job_related
        int application_id FK
        boolean llm_used
        int prompt_tokens
        int completion_tokens
        real estimated_cost_usd
        timestamp processed_at
    }
    
    scan_state {
        int id PK
        text email_account
        text email_folder
        int last_uid
        timestamp last_scan_at
    }
    
    applications ||--o{ status_history : tracks
    applications ||--o{ processed_emails : sourced_from
    scan_state ||--o{ processed_emails : scanned_by
```

### Table Details

**`applications`** â€” One row per unique job application

| Column | Type | Notes |
|--------|------|-------|
| `id` | INTEGER PK | Auto-increment |
| `company` | TEXT NOT NULL | Extracted company name |
| `job_title` | TEXT | Extracted job title |
| `email_subject` | TEXT | Original email subject |
| `email_sender` | TEXT | Sender address |
| `email_date` | TIMESTAMP | Email date in PT |
| `status` | TEXT NOT NULL | å·²ç”³è¯·, é¢è¯•, æ‹’ç», Offer, Unknown |
| `source` | TEXT | email / manual |
| `notes` | TEXT | User notes |
| `created_at` | TIMESTAMP | Row creation time |
| `updated_at` | TIMESTAMP | Last update time |

**Unique constraint**: `(company, job_title)` â€” prevents duplicate entries.

**`status_history`** â€” Audit trail of status changes

| Column | Type | Notes |
|--------|------|-------|
| `id` | INTEGER PK | Auto-increment |
| `application_id` | INTEGER FK | References applications.id |
| `old_status` | TEXT | Previous status |
| `new_status` | TEXT NOT NULL | New status |
| `change_source` | TEXT | email_uid / manual / llm |
| `changed_at` | TIMESTAMP | When the change occurred |

**`processed_emails`** â€” Every email scanned

| Column | Type | Notes |
|--------|------|-------|
| `id` | INTEGER PK | Auto-increment |
| `uid` | INTEGER NOT NULL | IMAP UID |
| `email_account` | TEXT NOT NULL | Which account |
| `email_folder` | TEXT | INBOX, etc. |
| `subject` | TEXT | Email subject |
| `sender` | TEXT | Sender |
| `email_date` | TIMESTAMP | Email date |
| `is_job_related` | BOOLEAN | Classification result |
| `application_id` | INTEGER FK | Linked application |
| `llm_used` | BOOLEAN | Whether LLM was used |
| `prompt_tokens` | INTEGER | LLM prompt tokens |
| `completion_tokens` | INTEGER | LLM completion tokens |
| `estimated_cost_usd` | REAL | Estimated LLM cost |
| `processed_at` | TIMESTAMP | When processed |

**Unique constraint**: `(uid, email_account, email_folder)` â€” prevents re-processing.

**`scan_state`** â€” Replaces `.job_monitor_state.json`

| Column | Type | Notes |
|--------|------|-------|
| `id` | INTEGER PK | Auto-increment |
| `email_account` | TEXT NOT NULL | Account identifier |
| `email_folder` | TEXT | Folder name |
| `last_uid` | INTEGER | Last processed UID |
| `last_scan_at` | TIMESTAMP | When last scan ran |

**Unique constraint**: `(email_account, email_folder)`.

---

## 5. Backend Module Design

### 5.1 Configuration â€” `config.py`

Pydantic Settings replaces manual `os.getenv()`:

```python
class AppConfig(BaseSettings):
    # IMAP
    imap_host: str
    imap_port: int = 993
    email_username: str
    email_password: SecretStr
    email_folder: str = "INBOX"
    # Database
    database_url: str = "sqlite:///job_monitor.db"
    # Scanning
    max_scan_emails: int = 20
    imap_timeout_sec: int = 30
    # LLM
    llm_enabled: bool = True
    llm_provider: str = "openai"
    llm_model: str = "gpt-4o-mini"
    llm_api_key: SecretStr = ""
    llm_timeout_sec: int = 45
    # Server
    host: str = "0.0.0.0"
    port: int = 8000
    # Logging
    log_level: str = "INFO"
```

### 5.2 FastAPI Server â€” `main.py`

```
FastAPI App
â”œâ”€â”€ /api/applications      # CRUD for applications
â”œâ”€â”€ /api/applications/{id} # Single application + history
â”œâ”€â”€ /api/scan              # Trigger email scan
â”œâ”€â”€ /api/stats             # Dashboard statistics
â”œâ”€â”€ /api/export            # Download CSV/Excel
â””â”€â”€ CORS middleware         # Allow React frontend
```

### 5.3 API Endpoints â€” `api/`

**`api/applications.py`**
| Method | Path | Description |
|--------|------|-------------|
| GET | `/api/applications` | List all (with filters: status, company, date range) |
| GET | `/api/applications/{id}` | Get one application + status history |
| POST | `/api/applications` | Manually add an application |
| PATCH | `/api/applications/{id}` | Update status or notes |
| DELETE | `/api/applications/{id}` | Delete an application |

**`api/scan.py`**
| Method | Path | Description |
|--------|------|-------------|
| POST | `/api/scan` | Trigger an email scan |
| GET | `/api/scan/status` | Get last scan info |

**`api/stats.py`**
| Method | Path | Description |
|--------|------|-------------|
| GET | `/api/stats` | Return counts by status, recent activity, LLM cost totals |

**`api/export.py`**
| Method | Path | Description |
|--------|------|-------------|
| GET | `/api/export?format=csv` | Download CSV |
| GET | `/api/export?format=excel` | Download Excel |

### 5.4 Email Client â€” `email/client.py`

IMAP connection with **tenacity** retry:
- Retry on transient `IMAP4.error` and `socket.timeout` (3 retries, exponential backoff)
- Configurable timeout
- Context manager for proper cleanup

### 5.5 Email Parser â€” `email/parser.py`

Extracted from current functions:
- `decode_mime_text()` â€” MIME header decoding
- `extract_body_text()` â€” multipart body extraction
- `is_noise_text()` â€” CSS/HTML junk detection

### 5.6 Classifier â€” `email/classifier.py`

- `is_job_related(subject, sender)` â€” keyword-based detection
- Configurable keyword lists

### 5.7 Extraction Rules â€” `extraction/rules.py`

All existing regex patterns, organized:
- `extract_company(subject, sender)` â€” company name extraction
- `extract_job_title(subject, body)` â€” job title extraction
- `extract_status(subject, body)` â€” status inference

### 5.8 LLM Provider â€” `extraction/llm.py`

Protocol-based abstraction for swappable LLM backends:

```
LLMProvider (Protocol)
â”œâ”€â”€ extract_fields(sender, subject, body) -> ExtractionResult

OpenAIProvider(LLMProvider)     # Current implementation
# Future:
AnthropicProvider(LLMProvider)
OllamaProvider(LLMProvider)
```

### 5.9 Extraction Pipeline â€” `extraction/pipeline.py`

Orchestrates rules + LLM:

```mermaid
flowchart TD
    A[Email Received] --> B{LLM Enabled?}
    B -->|Yes| C[LLM Extract Fields]
    C --> D{LLM Success?}
    D -->|Yes| E{Is Job Related?}
    D -->|No| F[Fallback to Rules]
    E -->|Yes| G[Merge LLM + Rule Results]
    E -->|No| H[Skip Email]
    B -->|No| I{Keyword Match?}
    I -->|Yes| F
    I -->|No| H
    F --> J[Rule-based Extraction]
    J --> K[Return Extracted Fields]
    G --> K
```

### 5.10 CLI â€” `cli.py`

Typer-based CLI with subcommands (also available alongside the web server):

```bash
job-monitor scan                    # Scan emails
job-monitor list                    # List applications
job-monitor export --format csv     # Export data
job-monitor serve                   # Start web server
job-monitor init                    # Initialize database
job-monitor migrate                 # Migrate from old format
```

---

## 6. Frontend Design (React + Vite + Tailwind)

### 6.1 Tech Stack

| Tool | Purpose |
|------|---------|
| **React 18** | UI framework |
| **TypeScript** | Type safety |
| **Vite** | Fast dev server + build |
| **Tailwind CSS** | Utility-first styling |
| **React Router** | Page routing |
| **TanStack Table** | Sortable/filterable data table |
| **Recharts** | Simple charts for stats |

### 6.2 Pages and Components

**Dashboard Page** â€” Main view at `/`

```mermaid
flowchart TB
    subgraph Dashboard
        SC[Stats Cards<br/>Total / Applied / Interview / Offer / Rejected]
        FB[Filter Bar<br/>Status dropdown / Company search / Date range]
        AT[Application Table<br/>Sortable columns / Status badges / Click to detail]
        SB[Scan Button<br/>Trigger scan + show progress]
        CH[Status Chart<br/>Pie or bar chart of statuses]
    end
    SC --> FB --> AT
    SB -.-> AT
    CH -.-> SC
```

**Stats Cards**: Show at-a-glance numbers
- Total Applications
- By Status: å·²ç”³è¯· / é¢è¯• / Offer / æ‹’ç»

**Filter Bar**: 
- Status dropdown filter
- Company name search
- Date range picker

**Application Table** columns:
| Column | Features |
|--------|----------|
| Company | Sortable, searchable |
| Job Title | Sortable |
| Status | Color-coded badge (green=Offer, blue=é¢è¯•, gray=å·²ç”³è¯·, red=æ‹’ç») |
| Date | Sortable, formatted to PT |
| Email Subject | Truncated with tooltip |
| Actions | Edit status, view detail, delete |

**Application Detail Page** â€” at `/applications/{id}`
- Full application info
- Status history timeline
- Edit status / add notes
- Related processed emails

### 6.3 UI Wireframe

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job Application Monitor                    [Scan Now]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  42  â”‚  â”‚  28  â”‚  â”‚   8  â”‚  â”‚   3  â”‚  â”‚   3  â”‚     â”‚
â”‚  â”‚Total â”‚  â”‚å·²ç”³è¯·â”‚  â”‚ é¢è¯• â”‚  â”‚Offer â”‚  â”‚ æ‹’ç» â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                          â”‚
â”‚  Status: [All â–¼]  Company: [________]  Date: [__ to __] â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Company  â”‚Job Title â”‚ Status â”‚  Date  â”‚  Actions   â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Google   â”‚ SWE III  â”‚ ğŸŸ¢ é¢è¯•â”‚ 02-15  â”‚ âœï¸  ğŸ—‘ï¸     â”‚ â”‚
â”‚  â”‚ Meta     â”‚ MLE      â”‚ âšª å·²ç”³è¯·â”‚ 02-14  â”‚ âœï¸  ğŸ—‘ï¸     â”‚ â”‚
â”‚  â”‚ Amazon   â”‚ SDE II   â”‚ ğŸ”´ æ‹’ç»â”‚ 02-12  â”‚ âœï¸  ğŸ—‘ï¸     â”‚ â”‚
â”‚  â”‚ Stripe   â”‚ Backend  â”‚ ğŸŸ¢ Offerâ”‚ 02-10  â”‚ âœï¸  ğŸ—‘ï¸     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚
â”‚  [< 1 2 3 >]                          [Export CSV â–¼]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Data Flow â€” Main Scan Workflow

```mermaid
flowchart TD
    A[Trigger Scan<br/>via CLI or Web Button] --> B[Load Config]
    B --> C[Init Database]
    C --> D[Get Scan State from DB]
    D --> E[Connect IMAP with retry]
    E --> F[Fetch UIDs after last_uid]
    F --> G{For each UID}
    G --> H{Already in processed_emails?}
    H -->|Yes| G
    H -->|No| I[Fetch and Parse Email]
    I --> J[Run Extraction Pipeline]
    J --> K{Job Related?}
    K -->|No| L[Record as non-job]
    K -->|Yes| M[Get or Create Application]
    M --> N[Update Status if Changed]
    N --> O[Record Status History]
    O --> P[Record in processed_emails]
    P --> G
    L --> G
    G -->|Done| Q[Update Scan State]
    Q --> R[Return Summary to Caller]
```

---

## 8. Technology Stack

| Component | Current | Proposed | Rationale |
|-----------|---------|----------|-----------|
| Config | `os.getenv()` | **pydantic-settings** | Validation, types, .env loading |
| Database | JSON + Numbers.app | **SQLAlchemy + SQLite** | Cross-platform, queryable, migratable |
| Migrations | None | **Alembic** | Schema versioning |
| Backend API | None | **FastAPI** | Async, auto-docs, Pydantic integration |
| Frontend | None | **React + Vite + Tailwind** | Modern, fast, lightweight |
| CLI | `if __name__` | **Typer** | Subcommands, help, shell completion |
| Logging | `print()` | **structlog** | Levels, structured JSON output |
| Retry | None | **tenacity** | IMAP/LLM transient failure recovery |
| LLM | OpenAI hardcoded | **Provider protocol** | Swappable backends |
| Export | Numbers only | **openpyxl + csv** | Cross-platform |
| Testing | None | **pytest + httpx** | Unit + API integration tests |
| Linting | None | **ruff** | Fast linting + formatting |
| Packaging | requirements.txt | **pyproject.toml** | Modern Python packaging |
| Containers | None | **Docker + compose** | Reproducible deployment |

---

## 9. Migration Strategy

One-time migration from current system:

1. **Export Numbers data** â€” Read `job_application_tracker.numbers` â†’ CSV
2. **Import into SQLite** â€” Populate `applications` table
3. **Transfer state** â€” Read `.job_monitor_state.json` â†’ `scan_state` table
4. **CLI command** â€” `job-monitor migrate` automates this

---

## 10. Implementation Phases

### Phase 1: Foundation
- Project structure with `pyproject.toml` and Makefile
- `config.py` with Pydantic settings
- `database.py` + `models.py` with SQLAlchemy ORM
- Alembic migration setup
- `logging_config.py` with structlog
- Tests for config and database

### Phase 2: Core Email Pipeline
- `email/parser.py` â€” extracted from existing code
- `email/classifier.py` â€” extracted from existing code
- `email/client.py` â€” IMAP with tenacity retry
- Tests for parser, classifier, client

### Phase 3: Extraction Engine
- `extraction/rules.py` â€” existing regex patterns refactored
- `extraction/llm.py` â€” OpenAI provider with protocol abstraction
- `extraction/pipeline.py` â€” orchestrator
- Tests for all extraction modules

### Phase 4: FastAPI Backend
- `main.py` â€” FastAPI app with CORS
- `schemas.py` â€” Pydantic request/response models
- `api/applications.py` â€” CRUD endpoints
- `api/scan.py` â€” trigger scan endpoint
- `api/stats.py` â€” statistics endpoint
- `api/export.py` â€” CSV/Excel download
- API tests with httpx

### Phase 5: React Frontend
- Vite + React + TypeScript + Tailwind setup
- API client module
- Dashboard page with stats cards
- Application table with sorting and filtering
- Filter bar component
- Status badges and chart
- Application detail page with history
- Scan button with progress feedback
- Export button

### Phase 6: CLI and Export
- `cli.py` with Typer â€” scan, list, export, serve, init, migrate
- `export/csv_export.py` and `export/excel_export.py`
- Tests for CLI and export

### Phase 7: Polish and Deploy
- Dockerfile + docker-compose.yml (backend + frontend + SQLite volume)
- `.gitignore` and comprehensive `README.md`
- Ruff linting configuration
- Migration command for existing data
- End-to-end integration tests
